---
title: "Week 1"
author: "C. Kalinowski"
date: "3/1/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,comment="",cache=TRUE)
```

# Task 0 - Understanding the Problem

## Download the data

The data was downloaded and unzipped. It contains a folder named "final" with 4 subfolders.

- German (de_DE)
- English (en_US)
- Finnish (fi_FI)
- Russian (ru_RU)

Each folder contains 3 files beginning with the folder name and ending with the source name.

- blogs (.blogs.txt)
- news (.news.txt)
- twitter (.twitter.txt)

The path to Russian blogs is "../Data/final/ru_RU/ru_RU.blogs.txt"

## Load the data into R

readLines outputs a character vector of length y, and you can access specific elements with the square brackets.

fileReader is a basic first pass at a function that takes a file name x and a length y. It opens a connection to file x for read purposes, inputs y lines. It then closes the connection and returns a character vector of length y. 

```{R}

sampleReader<-function(x,y=-1L,z="en_US"){
      ## Reads a specified number of lines from an input text from the data set
      ## First paste together file name from input x (either twitter, blogs, or news)
      ## Assume language is english, but can be switched to de, ru, or fi with z arg
      ## Open file connection, input y lines, or all if not specified
      ## close connection, return input lines as character vector of length y

   filename<-paste("../Data/final/",z,"/",z,".",x,".txt",sep="")
   con<-file(filename,"r")
   inputtext<-readLines(con,n=y)
   close(con)
   inputtext
}

myline<-sampleReader("twitter",2)
myline

```


# Task 1 - Getting and Cleaning the Data

## Tokenization

Tokenization is splitting lines into words. A simple tokenizer, according to Jurafsky's lectures, splits on non-alpha characters.

A basic tokenizer is found in the tau package. It simply splits based on whitespace.

```{R}

library(tau)
head(tokenize(myline))

```

A better option is probably the boost tokenizer in tm, which keeps punctuation...

```{R}

library(tm)
head(Boost_tokenizer(myline))

```


...or the MC tokenizer, which doesn't.

```{R}

head(MC_tokenizer(myline))

```

## Profanity Removal

Probably the best way to deal with this is to use an already-generated list of words like the google bad words list. There is a 2010 list available at "https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/badwordslist/badwords.txt", which was downloaded and stored as "../Data/badwords.txt"

The following "clean" tokenizer takes a character vector as input. It tokenizes it using the MC tokenizer, and then compares it against the bad words list. Anything in the tokenized vector that is on the list is replaced with the string "PRFNTY", and the "cleaned" vector is returned as output.

```{R}

cleanToken<-function(x){
      ## creates "clean" set of tokens from input x
      ## use MC_tokenizer to create token list
      ## convert to all lowercase
      ## check token list against badwords list
      ## replace tokens on the bad list with "PRFNTY"
      ## remove all punctuation marks
      ## replace http(s) website addresses with WBDDRSS
      ## return token list

   token<-MC_tokenizer(x)
   badWordsList<-readLines("../Data/badwords.txt",n=-1L,warn=FALSE)
   token<-tolower(token)
   checkToken<-token %in% badWordsList
   token[checkToken]<-"PRFNTY"
   token<-gsub("[^A-Za-z0-9]+$","",token)
   token<-gsub("^[^A-Za-z0-9]*$","",token)
   token<-gsub("[Hh][Tt][Tt][Pp][^ ]+","WBDDRSS",token)
   token
}

myclean1<-cleanToken(myline[1])
myclean1

```