---
title: "Week 2"
author: "C. Kalinowski"
date: "3/1/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,comment="",cache=TRUE,warning = FALSE)

library(tm)
library(ggplot2)
library(data.table)

sampleReader<-function(x,y=-1L,z="en_US"){
      ## Reads a specified number of lines from an input text from the data set
      ## First paste together file name from input x (either twitter, blogs, or news)
      ## Assume language is english, but can be switched to de, ru, or fi with z arg
      ## Open file connection, input y lines, or all if not specified
      ## close connection, return input lines as character vector of length y

   filename<-paste("../Data/final/",z,"/",z,".",x,".txt",sep="")
   con<-file(filename,"r")
   inputtext<-readLines(con,n=y)
   close(con)
   inputtext
}

cleanToken<-function(x){
      ## creates "clean" set of tokens from input x
      ## use MC_tokenizer to create token list
      ## convert to all lowercase
      ## check token list against badwords list
      ## replace tokens on the bad list with "PRFNTY"
      ## remove all punctuation marks
      ## replace http(s) website addresses with WBDDRSS
      ## return token list

   token<-MC_tokenizer(x)
   badWordsList<-readLines("../Data/badwords.txt",n=-1L,warn=FALSE)
   token<-tolower(token)
   checkToken<-token %in% badWordsList
   token[checkToken]<-"PRFNTY"
   token<-gsub("[^A-Za-z0-9]+$","",token)
   token<-gsub("^[^A-Za-z0-9]*$","",token)
   token<-gsub("[Hh][Tt][Tt][Pp][^ ]+","WBDDRSS",token)
   token
}

```

# Task 2 - Exploratory Data Analysis

## Exploratory Analysis and Word Frequencies

Based on tokenization, we can explore the words in the corpus. A possible function would be to create a clean tokenized list of vectors, unlist the contents, and return a frequency table as a data table.

```{R}

tokenFreq<-function(x){
      ## creates data table of tokens from input text line x
      ## runs input x through cleanToken to get token list
      ## unlists token list to get single list
      ## creates data.table and calculates frequencies
      ## returns data.table

   words<-cleanToken(x)
   wordslist<-as.data.table(unlist(words))
   wordslist<-wordslist[,.(.N),keyby=wordslist]
   colnames(wordslist)<-c("token","freq")
   wordslist
}

sample1<-sampleReader("blogs",5)
sample1[4]
tokens1<-tokenFreq(sample1)
head(tokens1)

```

In each data set, a random sample of 200 lines has the following frequency distribution:

```{R}
set.seed=322021
twitter<-sampleReader("twitter")
sampleTwitter<-sample(twitter,size=200,replace=F)
rm(twitter)
blog<-sampleReader("blogs")
sampleBlog<-sample(blog,size=200,replace=F)
rm(blog)
news<-sampleReader("news")
sampleNews<-sample(news,size=200,replace=F)
rm(news)
twitterToken<-tokenFreq(sampleTwitter)
blogToken<-tokenFreq(sampleBlog)
newsToken<-tokenFreq(sampleNews)
hist(log(twitterToken$freq),main="Twitter Frequency Distribution",xlab="Frequencies",
   ylab="Distribution",breaks=20)
hist(log(subset(twitterToken,freq>2)$freq),main="Twitter Frequency Distributions greater than 2",
   xlab="Frequencies",ylab="Distribution",breaks=20)

```


The words with the highest counts in each of the 3 English corpora are:

```{R}
# BLOGS:
head(blogToken[order(-freq)])
# NEWS:
head(newsToken[order(-freq)])
# TWITTER:
head(twitterToken[order(-freq)])

```
## N-Gram Frequency

An easy way to create N-grams is to paste together token vectors.

```{R}

ngrammer<-function(x,y){
      ## Creates list of y-grams from input tokenized list x
      ## Checks whether y is greater than length of x, else returns NULL
      ## Creates y vectors with stepped start and end points
      ## Vector 1 starts at 1 and ends at length of x - y
      ## Vector y starts at y and ends at end of x
      ## Binds vectors into data frame
      ## Pastes rows of data frame with space separator
      ## Returns list of pasted rows

   ngramMatrix<-NULL
   size<-length(x)
   if(size<=y){
      return()
   }

   ngramMatrix<-matrix(nrow=(size-y+1),ncol=0)
   for (i in (1:y)){
      tokenlist<-x[i:(size-y+i)]
      ngramMatrix<-cbind(ngramMatrix,tokenlist)
   }
   
   df_args <- c(as.data.table(ngramMatrix), sep=" ")
   df_args<-as.data.table(do.call(paste, df_args))
}


sample1[4]
head(ngrammer(cleanToken(sample1[4]),4),10)

```

# Task 3 - Modeling

The first task for modeling is creating ngram frequencies from the test sets and assigning them probabilities. Each of the 3 English corpora were split 60-20-20 into training, validation, and test sets.


```{R,include=FALSE}
set.seed(3022021)
twitter<-sampleReader("twitter")
twittertest<-sample(1:length(twitter),size=length(twitter)*0.2,replace=FALSE)
testTwitter<-twitter[twittertest]

twittertrain<-twitter[-twittertest]
twittervalid<-sample(1:length(twittertrain),size=length(twittertrain)*0.25,replace=FALSE)
validTwitter<-twittertrain[twittervalid]
trainTwitter<-twittertrain[-twittervalid]

rm(twitter,twittertest,twittertrain,twittervalid)

blogs<-sampleReader("blogs")
blogstest<-sample(1:length(blogs),size=length(blogs)*0.2,replace=FALSE)
testBlogs<-blogs[blogstest]

blogstrain<-blogs[-blogstest]
blogsvalid<-sample(1:length(blogstrain),size=length(blogstrain)*0.25,replace=FALSE)
validBlogs<-blogstrain[blogsvalid]
trainBlogs<-blogstrain[-blogsvalid]

rm(blogs,blogstest,blogstrain,blogsvalid)

news<-sampleReader("news")
newstest<-sample(1:length(news),size=length(news)*0.2,replace=FALSE)
testNews<-news[newstest]

newstrain<-news[-newstest]
newsvalid<-sample(1:length(newstrain),size=length(newstrain)*0.25,replace=FALSE)
validNews<-newstrain[newsvalid]
trainNews<-newstrain[-newsvalid]

rm(news,newstest,newstrain,newsvalid)

```

```{R}

ngramtable<-function(x,y){
      ## Creates ngram frequency table of n-1 as rows and last word as column
      ## x is tokenized list, y is ngram number
      ## If y>2, send x to ngrammer function with y-1 for row generator
      ## If y=2, create vector using x from 1 to end-1 as first word generator
      ## Create vector using x from y to end as last word generator
      ## Create data frame with ngrammer or first word as first column 
      ## and last word as second column
      ## If z, create and return frequency table of ngrammer by last word
      ## If not z, return data table
   
   if(y>2){
      ngrams<-ngrammer(x,(y-1))
   }
   else{
      ngrams<-x[1:(length(x)-1)]
   }
   singles<-x[y:(length(x))]
   ngramtable<-data.table(ngrams[1:length(singles)],
                  single = singles)
   
   ngramtable<-ngramtable[,.(freq=.N),keyby = .(phrase=V1,single)]
   ngramtable
}

sampletable<-ngramtable(cleanToken(sample1[4]),4)
head(sampletable[order(-freq)])

```


For multi-line corpora, using lapply gets the correct output

```{R}
trainTwitter[1:2]
token<-lapply(trainTwitter[1:2],cleanToken)
grams<-lapply(token,ngrammer,y=4)
gramDT<-lapply(token[1:2],ngramtable,y=4)
gramList<-do.call("rbind",gramDT)
gramList

```

# Frequencies
## Unigrams

```{R,eval=FALSE}
# Clean and tokenize the twitter data
twitterClean<-lapply(trainTwitter,cleanToken)
# unlist the token list to create a single list
twitter1gramList<-unlist(twitterClean)
# create a frequency table
twitter1gramFreq<-table(twitter1gramList)
# create a probability table
twitter1gramProb<-round((twitter1gramFreq/length(twitter1gramList)*100),6)
```

## Bigrams

```{R,eval=FALSE}
library(dplyr)
# create 2gram dfs
twitter2grams<-lapply(twitterClean,ngramtable,y=2,z=FALSE)
# combine 2gram dfs
twitter2gramList<-do.call("rbind",twitter2grams)
twitter2gramList<-as.data.frame(twitter2gramList)
# create a frequency chart
twitter2gramFreq<-table(twitter2gramList[,1],twitter2gramList[,2])

```

*******************normalize frequencies by unigrams***********
frequency of "a b" / frequency of a