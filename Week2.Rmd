---
title: "Week 2"
author: "C. Kalinowski"
date: "3/1/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,comment="",cache=TRUE,warning = FALSE)
library(tm)
library(ggplot2)

sampleReader<-function(x,y=-1L,z="en_US"){
      filename<-paste("../Data/final/",z,"/",z,".",x,".txt",sep="")
      con<-file(filename,"rb")
      inputtext<-readLines(con,n=y)
      close(con)
      inputtext<-inputtext[!is.na(inputtext)]
      inputtext
}

cleanToken<-function(x){
      token<-MC_tokenizer(x)
      badWordsList<-readLines("../Data/badwords.txt",n=-1L,warn=FALSE)
      checkToken<-token %in% badWordsList
      token[checkToken]<-"PRFNTY"
      token<-gsub("[Hh][Tt][Tt][Pp][Ss]?:[^ ]+","WBDDRSS",token)
      token
}
```

# Task 2 - Exploratory Data Analysis

## Exploratory Analysis and Word Frequencies

Based on tokenization, we can explore the words in the corpus. A possible function would be to create a clean tokenized list of vectors, unlist the contents, and return a frequency table as a data frame.

```{R}

getTokens<-function(x){
      words<-cleanToken(x)
      wordslist<-unlist(words)
      as.data.frame(table(wordslist))
}

sample1<-sampleReader("blogs",5)
sample1[4]
tokens1<-getTokens(sample1)
head(tokens1)

```

In each data set, a random sample of 200 lines has the following frequency distribution:

```{R}
set.seed=322021
twitter<-sampleReader("twitter")
sampleTwitter<-sample(twitter,size=200,replace=F)
rm(twitter)
blog<-sampleReader("blogs")
sampleBlog<-sample(blog,size=200,replace=F)
rm(blog)
news<-sampleReader("news")
sampleNews<-sample(news,size=200,replace=F)
rm(news)
twitterToken<-getTokens(sampleTwitter)
blogToken<-getTokens(sampleBlog)
newsToken<-getTokens(sampleNews)
hist(log(twitterToken$Freq),main="Twitter Frequency Distribution",xlab="Frequencies",
     ylab="Distribution",breaks=20)
hist(log(subset(blogToken,Freq>2)$Freq),main="Twitter Frequency Distributions greater than 2",
     xlab="Frequencies",ylab="Distribution",breaks=20)

```


The words with the highest counts in each of the 3 English corpora are:

```{R}
# BLOGS:
maxBlog<-subset(blogToken,Freq %in% head(sort(blogToken$Freq,decreasing=TRUE),10))
maxBlog[order(maxBlog$Freq,decreasing=TRUE),]
# NEWS:
maxNews<-subset(newsToken,Freq %in% head(sort(newsToken$Freq,decreasing=TRUE),10))
maxNews[order(maxNews$Freq,decreasing=TRUE),]
# TWITTER:
maxTwitter<-subset(twitterToken,Freq %in% head(sort(twitterToken$Freq,decreasing=TRUE),10))
maxTwitter[order(maxTwitter$Freq,decreasing=TRUE),]

```
## N-Gram Frequency

An easy way to create N-grams is to paste together token vectors.

```{R}

ngrammer<-function(x,y,table=FALSE){
   ngramMatrix<-NULL
   size<-length(x)
   if(size<=y){
      return()
   }
   ngramMatrix<-matrix(nrow=(size-y+1),ncol=0)
   for (i in (1:y)){
      tokenlist<-x[i:(size-y+i)]
      ngramMatrix<-cbind(ngramMatrix,tokenlist)
   }
   df_args <- c(as.data.frame(ngramMatrix), sep=" ")
   do.call(paste, df_args)
}

sample1[4]
head(ngrammer(cleanToken(sample1[4]),4),10)

```

\pagebreak   
*********************************** Turn ngram list into frequency table **************
# Task3 - Modeling

The first task for modeling is creating ngram frequencies from the test sets and assigning them probabilities. Each of the 3 English corpora were split 60-20-20 into training, validation, and test sets.


```{R,include=FALSE}
set.seed(3022021)
twitter<-sampleReader("twitter")
twittertest<-sample(1:length(twitter),size=length(twitter)*0.2,replace=FALSE)
testTwitter<-twitter[twittertest]

twittertrain<-twitter[-twittertest]
twittervalid<-sample(1:length(twittertrain),size=length(twittertrain)*0.25,replace=FALSE)
validTwitter<-twittertrain[twittervalid]
trainTwitter<-twittertrain[-twittervalid]

rm(twitter,twittertest,twittertrain,twittervalid)

blogs<-sampleReader("blogs")
blogstest<-sample(1:length(blogs),size=length(blogs)*0.2,replace=FALSE)
testBlogs<-blogs[blogstest]

blogstrain<-blogs[-blogstest]
blogsvalid<-sample(1:length(blogstrain),size=length(blogstrain)*0.25,replace=FALSE)
validBlogs<-blogstrain[blogsvalid]
trainBlogs<-blogstrain[-blogsvalid]

rm(blogs,blogstest,blogstrain,blogsvalid)

news<-sampleReader("news")
newstest<-sample(1:length(news),size=length(news)*0.2,replace=FALSE)
testNews<-news[newstest]

newstrain<-news[-newstest]
newsvalid<-sample(1:length(newstrain),size=length(newstrain)*0.25,replace=FALSE)
validNews<-newstrain[newsvalid]
trainNews<-newstrain[-newsvalid]

rm(news,newstest,newstrain,newsvalid)

```

```{R}

twitter4gram<-ngrammer(cleanToken(trainTwitter),y=4)

```